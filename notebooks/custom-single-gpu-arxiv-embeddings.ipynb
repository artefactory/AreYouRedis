{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T17:00:38.727567Z",
     "iopub.status.busy": "2021-10-26T17:00:38.727285Z",
     "iopub.status.idle": "2021-10-26T17:00:38.730682Z",
     "shell.execute_reply": "2021-10-26T17:00:38.730051Z",
     "shell.execute_reply.started": "2021-10-26T17:00:38.727541Z"
    },
    "tags": []
   },
   "source": [
    "# arXiv Paper Embedding\n",
    "\n",
    "## On a Single GPU\n",
    "This notebook utilizes 1 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install semanticscholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cudf\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import src.scholar_citations as sch_script\n",
    "import src.vectors as vect\n",
    "\n",
    "DATA_PATH = \"./arxiv-metadata-oai-snapshot.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data pre processing\n",
    "Before we do anything else, we need to load the papers dataset, do some basic cleaning, and get it into a workable format. Below,\n",
    "we will use CuDF to house the data and apply seom transformations in a generator, loading from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_description(description: str):\n",
    "    if not description:\n",
    "        return \"\"\n",
    "    # remove unicode characters\n",
    "    description = description.encode('ascii', 'ignore').decode()\n",
    "\n",
    "    # remove punctuation\n",
    "    description = re.sub('[%s]' % re.escape(string.punctuation), ' ', description)\n",
    "\n",
    "    # clean up the spacing\n",
    "    description = re.sub('\\s{2,}', \" \", description)\n",
    "\n",
    "    # remove urls\n",
    "    #description = re.sub(\"https*\\S+\", \" \", description)\n",
    "\n",
    "    # remove newlines\n",
    "    description = description.replace(\"\\n\", \" \")\n",
    "\n",
    "    # remove all numbers\n",
    "    #description = re.sub('\\w*\\d+\\w*', '', description)\n",
    "\n",
    "    # split on capitalized words\n",
    "    description = \" \".join(re.split('(?=[A-Z])', description))\n",
    "\n",
    "    # clean up the spacing again\n",
    "    description = re.sub('\\s{2,}', \" \", description)\n",
    "\n",
    "    # make all words lowercase\n",
    "    description = description.lower()\n",
    "\n",
    "    return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = vect.load_raw_data(DATA_PATH)\n",
    "data = pd.DataFrame.from_records(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.categories = data.categories.str.lower()\n",
    "condition = (\n",
    "    (data.categories.str.contains('cs.lg')) |\n",
    "    (data.categories.str.contains('cs.ai')) |\n",
    "    (data.categories.str.contains('cs.cl')) |\n",
    "    (data.categories.str.contains('cs.cv')) |\n",
    "    (data.categories.str.contains('cs.ne')) \n",
    ")\n",
    "data_ds = data[condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_categories(cat_str: str):\n",
    "    cat_str = [elem for elem in cat_str.split(\" \") if elem]\n",
    "    return \", \".join(cat_str)\n",
    "data_ds['categories'] = [get_categories(x) for x in data_ds.categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_ds = sch_script.fill_year_column(data_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ = data_ds['title'] + data_ds['abstract']\n",
    "data_ds['input'] =  input_.apply(lambda x: clean_description(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = cudf.DataFrame.from_pandas(data_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(cdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cdf.year.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2: Create sentence embeddings\n",
    "Here I use a cookie-cutter -- **out of the box** -- model from HuggingFace to transform papers abstracts + titles into vectors.\n",
    "\n",
    "**This takes a long time**... So best to take a subset. Or use the dask cluster for multi-gpu encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = cdf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change if needed\n",
    "sample = len(cdf)\n",
    "batch = cdf[:sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "vectors = model.encode(\n",
    "    sentences = batch.input.values_host,\n",
    "    normalize_embeddings = True,\n",
    "    batch_size = 64,\n",
    "    show_progress_bar = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vectors created!\n",
    "batch['vector'] = cudf.Series(vectors.tolist(), index=batch.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch.to_json(\"ds_embeddings.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get papers citations using Semantic Scholar API requests : please add api_key in `src.scholar_citations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ids = pd.read_json(\"./ds_embeddings.json\").id\n",
    "ids = batch.id.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "items = list(zip(\n",
    "    ids,\n",
    "    [\"./citations.json\" for i in range(len(ids))],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with multiprocessing.Pool() as pool:\n",
    "    for res in tqdm(pool.starmap(sch_script.get_sch_paper, items)):\n",
    "        results.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean citations and get missing/error due to deconnections : run this part few times until all ids are there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [file for file in os.listdir('./') if (\"citations_\" in file) or (\"missing_ids\" in file)]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "messy = []\n",
    "\n",
    "for file in files:\n",
    "    with open(file, \"r\") as f:\n",
    "        for jsonObj in f:\n",
    "            try:\n",
    "                dd = json.loads(jsonObj)\n",
    "                data.append(dd)\n",
    "            except:\n",
    "                messy.append(jsonObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame.from_records(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ids = set(emb.id) - set(data.arxiv_id)\n",
    "len(missing_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_missing = []\n",
    "items = list(zip(\n",
    "    missing_ids,\n",
    "    [\"./missing_ids.json\" for i in range(len(missing_ids))],\n",
    "    )\n",
    ")\n",
    "\n",
    "with multiprocessing.Pool() as pool:\n",
    "    for res in tqdm(pool.starmap(sch_script.get_sch_paper, items)):\n",
    "        results_missing.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't forget to add the new `results_missing` to data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final step to : clean and get data with vectors and citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset=['arxiv_id']\n",
    "data = data.dropna(subset=['sch_id'])\n",
    "data = data.rename(columns={\"arxiv_id\": \"id\"})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_embeddings = pd.merge(emb, data, on='id', how='inner')\n",
    "len(merged_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_embeddings.to_json('./final_redis_ds_data.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
